<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Special Session @ IJCNN 2025</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="banner">
        <div class="logo-container">
            <a href="https://www.ieee.org/" target="_blank">
                <img src="images/ieee_logo_white.png" alt="Special Session Logo">
            </a>
        </div>
        <div class="session-info">
            <h1>
                Special Session @ 
                <a href="https://2025.ijcnn.org/" target="_blank">
                    <img src="images/ijcnn_simple.png" alt="IJCNN 2025 Logo" style="height: 30px; vertical-align: middle;">
                </a>
            </h1>
        </div>        
    </div>
    <!-- Second Banner with a Large Image -->
    <div class="second-banner">
        <img src="images/ijcnn_large.png" alt="Large Banner Image">
    </div>
    
    <!-- Orange Horizontal Line (HR) -->
    <hr class="orange-line">

    <!-- Proposal Text -->
    <div class="proposal-info">
        <h2>Special Session: Data-efficient Vision Transformers: Challenges & Applications</h2>
        <p>International Joint Conference on Neural Networks (IJCNN) <a href="https://2025.ijcnn.org/" target="_blank">https://2025.ijcnn.org/</a> <br>Rome, Italy <br> 30 June â€“ 5 July, 2025</p>

    </div>

    <!-- DESCRIPTION Section -->
    <div class="section" id="description">
        <h2>Description</h2>
        <p>Vision Transformers (ViTs) have revolutionised computer vision, offering state-of-the-art performance across a range of tasks. However, their heavy dependence on large datasets poses significant challenges, particularly in scenarios where data collection is constrained due to cost, privacy, or resource limitations. This special session on Data-efficient Vision Transformers: Challenges & Applications addresses a critical gap by exploring innovative methods to make ViTs viable in data-scarce environments, aligning closely with IJCNN 2025's emphasis on advancing neural networks for real-world applicability.</p>
        <p>The special session will explore cutting-edge techniques such as self-supervised learning, hybrid models, and generative augmentation strategies to reduce data reliance. By bringing together researchers and practitioners, it aims to foster discussions on creating lightweight, efficient ViT architectures optimised for diverse domains, including healthcare, autonomous systems, and smart cities. Furthermore, the focus on practical applications highlights the real-world relevance of these advancements, ensuring the outcomes of this session resonate beyond academia.</p>
        <p>Incorporating contributions from leading experts in AI and vision transformers, this special session uniquely emphasises the intersection of technical innovation and practical deployment. The novel combination of self-supervised approaches, generative modelling, and data-efficient model design ensures its potential to drive impactful solutions for critical sectors. By addressing both foundational challenges and emerging applications, the session promises to push the boundaries of current research, offering transformative insights for participants.</p>
        <p>This special session closely aligns with the themes of IJCNN 2025 but also extends its scope by fostering cross-disciplinary discussions and collaborations. The outcomes are expected to shape future research, industry practices, and policymaking, ensuring that ViTs are accessible and effective in scenarios with limited resources. With its focus on data efficiency, the session will catalyse advancements in deploying AI for global challenges, embodying the transformative potential of neural networks.</p>
    </div>


    <hr class="orange-line">

    <!-- List of Topics Section -->
    <div class="section" id="topics">
        <h2>List of Topics</h2>
        <p>Contributions are expected to be related, but not limited, to the following topics:</p>
        <ul>
            <li>AI-driven medical decision support with Vision Transformers</li>
            <li>Biomedical and health informatics using data-efficient Vision Transformers</li>
            <li>Computer-aided disease detection, diagnosis, and prognosis with Vision Transformers</li>
            <li>Public health informatics with data-efficient AI models</li>
            <li>Explainable AI in healthcare applications using Vision Transformers</li>
            <li>Fuzzy modeling and Vision Transformers for intelligent healthcare systems</li>
            <li>Advanced neural network architectures in healthcare, focusing on Vision Transformers</li>
            <li>Transfer learning, multitask learning, and multi-view learning using Vision Transformers in healthcare</li>
            <li>Machine learning and deep learning approaches for medical imaging, ECG, EMG, EEG data using Vision Transformers</li>
            <li>Natural language processing for medical science using Vision Transformers</li>
            <li>Real-time monitoring and patient tracking with Vision Transformers</li>
            <li>Privacy-aware AI architectures in healthcare with decentralized approaches (e.g., federated learning) for Vision Transformers</li>
        </ul>
    </div>

    <hr class="orange-line">

    <!-- Audience Section -->
    <div class="section" id="audience">
        <h2>Audience</h2>
        <p>This special session is tailored for researchers, practitioners, and industry professionals working with Vision Transformers (ViTs) and their applications in data-limited scenarios. The session specifically addresses the challenges of deploying ViTs in real-world environments, particularly in fields where data collection is constrained, such as healthcare, autonomous systems, and smart cities.</p>
        <p>The session will focus on innovative techniques like self-supervised learning, hybrid models, and generative data augmentation strategies, and is designed for those interested in making ViTs more efficient in resource-constrained environments.</p>
        <p>We invite contributions from a broad range of stakeholders, including academic researchers, data scientists, AI practitioners, healthcare professionals, and industry leaders. The session encourages interdisciplinary collaboration, aiming to bridge the gap between cutting-edge AI research and its real-world applications to address global challenges.</p>
        <p><strong>Keywords:</strong> Vision Transformers, self-supervised learning, generative models, AI applications, data-efficient AI, healthcare, smart cities, autonomous systems, machine learning.</p>
    </div>    

    <hr class="orange-line">

    <!-- Submission Guidelines Section -->
    <div class="section" id="submission-guidelines">
        <h2>Submission Guidelines</h2>
        <p>Please follow the instructions carefully for submitting your paper to IJCNN 2025:</p>
        <ul>
            <li><strong style="color: red;">Double-Blind Reviewing:</strong> <span style="font-weight: bold;">Ensure anonymity by removing author names and references to prior work in the first person. Papers revealing author identities may be rejected.</span></li>
        <li><strong style="color: red;">Submission System:</strong> <span style="font-weight: bold;">Papers must be submitted through the IEEE IJCNN 2025 online submission system. Special session papers must select the corresponding title 'Data-efficient Vision Transformers: Challenges & Applications' during submission.</span></li>
            <li><strong>AI-Generated Text Disclosure:</strong> If using AI-generated text, disclose it in the acknowledgements and cite the AI system used.</li>
            <li><strong>Plagiarism Check:</strong> Papers will be checked for plagiarism and may be desk-rejected if suspected of plagiarism.</li>
            <li><strong>Paper Length:</strong> Each paper is limited to 8 pages. Up to 2 additional pages are allowed for a fee of $100 per extra page.</li>            <li><strong>Submission System:</strong> IJCNN 2025 uses Microsoft CMT for paper submission. The system is available at <a href="https://cmt3.research.microsoft.com/IJCNN2025/" target="_blank">Microsoft CMT</a>.</li>
        </ul>
        <p>For more details, visit <a href="https://2025.ijcnn.org/authors/initial-author-instructions" target="_blank">https://2025.ijcnn.org/authors/initial-author-instructions</a>.</p>
    </div>


    <hr class="orange-line">

    <!-- Important Dates Section -->
    <div class="section" id="important-dates">
        <h2>Important Dates</h2>
        <ul>
            <li><strong>15 January 2025:</strong> Paper Submission Deadline</li>
            <li><strong>31 March 2025:</strong> Paper Acceptance Notification</li>
            <li><strong>1 May 2025:</strong> Camera-Ready Paper Submission Deadline</li>
            <li><strong>1 May 2025:</strong> Early Registration Deadline</li>
            <li><strong>TBA:</strong> Special Session: Data-efficient Vision Transformers</li>
        </ul>
    </div>


    <hr class="orange-line">

   <!-- Organizers Section -->
    <div class="section" id="organizers">
        <h2>Organizers</h2>
        <div class="organizers-container">
            <div class="organizer">
                <img src="images/RAZAH72409.jpg" alt="Organizer 1" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Dr. Haider Raza</p>
                    <p class="affiliation">University of Essex, UK</p>
                </div>
            </div>
            <div class="organizer">
                <img src="images/Muhammad-Haris-Khan-secondary.jpg" alt="Organizer 2" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Dr. Muhammad Haris Khan</p>
                    <p class="affiliation">MBZUAI, UAE</p>
                </div>
            </div>
            <div class="organizer">
                <img src="images/GANJO00207.jpg" alt="Organizer 3" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Prof. John Q Gan</p>
                    <p class="affiliation">University of Essex, UK</p>
                </div>
            </div>
            <div class="organizer">
                <img src="images/Mohsin.jpeg" alt="Organizer 4" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Mohsin Ali</p>
                    <p class="affiliation">University of Essex, UK</p>
                </div>
            </div>
        </div>
    </div>


    <hr class="orange-line">

    <!-- Contact Section -->
    <div class="section" id="contact">
        <h2>Contact</h2>
        <p>For any inquiries please email us at: <a href="mailto:h.raza@essex.ac.uk">h.raza@essex.ac.uk</a></p>
        <p>This special session is co-located at the IJCNN 2025. More information can be found on the official website of the conference: 
            <a href="https://2025.ijcnn.org/" target="_blank">https://2025.ijcnn.org/</a>
        </p>
    </div>


    <hr class="orange-line">

   <!-- Institutes Section -->
    <div class="section" id="institutes">
        <h2>Institutes</h2>
        <div class="institutes-container">
            <div class="institute">
                <img src="images/University_of_Essex_logo.png" alt="University of Essex" class="institute-img">
                <div class="institute-info">
                    <p class="name">University of Essex</p>
                    <p class="affiliation">Colchester, United Kingdom</p>
                </div>
            </div>
            <div class="institute">
                <img src="images/mbzua_logo.png" alt="MBZUAI" class="institute-img">
                <div class="institute-info">
                    <p class="name">MBZUAI</p>
                    <p class="affiliation">Abu Dhabi, United Arab Emirates</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Footer Section for Copyright -->
    <footer class="copyright-banner">
        <p>&copy; 2025 Special Session @ IJCNN. All Rights Reserved.</p>
    </footer>

</body>
</html>
