<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Special Session @ IJCNN 2025</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="banner">
        <div class="logo-container">
            <a href="https://www.ieee.org/" target="_blank">
                <img src="images/ieee_logo_white.png" alt="Special Session Logo">
            </a>
        </div>
        <div class="session-info">
            <h1>
                Special Session @ 
                <a href="https://2025.ijcnn.org/" target="_blank">
                    <img src="images/ijcnn_simple.png" alt="IJCNN 2025 Logo" style="height: 30px; vertical-align: middle;">
                </a>
            </h1>
        </div>        
    </div>
    <!-- Second Banner with a Large Image -->
    <div class="second-banner">
        <img src="images/ijcnn_large.png" alt="Large Banner Image">
    </div>
    
    <!-- Orange Horizontal Line (HR) -->
    <hr class="orange-line">

    <!-- Proposal Text -->
    <div class="proposal-info">
        <h2>Special Session: Data-efficient Vision Transformers: Challenges & Applications</h2>
        <p>International Joint Conference on Neural Networks (IJCNN) <a href="https://2025.ijcnn.org/" target="_blank">https://2025.ijcnn.org/</a> <br>Rome, Italy <br> 30 June – 5 July, 2025</p>

    </div>

    <!-- DESCRIPTION Section -->
    <div class="section" id="description">
        <h2>Overview</h2>
        <p>Vision Transformers (ViTs) have emerged as a breakthrough architecture in the field of computer vision, achieving state-of-the-art results across a variety of tasks. However, ViTs are data-hungry, often requiring large datasets to achieve optimal performance. This has led to challenges in data-scarce scenarios, where collecting and labelling large amounts of training data is impractical. The "Data-efficient Vision Transformers" workshop aims to explore new techniques, challenges, and applications that address the issue of data efficiency in Vision Transformers.</p>
        <p>This workshop will gather experts from academia, industry, and research institutions to discuss cutting-edge solutions for improving the data efficiency of ViTs, such as novel training setups and architecture, self-supervised learning, transfer learning, and augmentation strategies. The goal is to bridge the gap between the high-performance potential of ViTs and their real-world applicability in scenarios with limited training data.</p>
        </div>


    <hr class="orange-line">


    
    <div class="section" id="objectives">
        <h2>Objectives</h2>
        <p>The workshop will aim to:</p>
        <ul>
            <li>Identify challenges in deploying Vision Transformers (ViTs) with limited training data.</li>
            <li>Present recent advances and research findings on data-efficient vision transformer techniques.</li>
            <li>Foster collaboration between researchers and industry professionals working on ViTs in data-scarce environments.</li>
            <li>Highlight real-world applications of data-efficient ViTs and showcase successful case studies.</li>
            <li>Encourage open discussions on potential solutions, future directions, and research opportunities.</li>
            <li>Demonstrate the broader relevance of this workshop by appealing to audiences beyond the typical scope of IJCNN, including professionals from healthcare, autonomous systems, smart cities, and more.</li>
        </ul>
    </div>
    <hr class="orange-line">


    <!-- List of Topics Section -->
    <div class="section" id="topics">
        <h2>Scope and Topics</h2>
        <p>As the field of artificial intelligence continues to advance, the deployment of deep learning models, especially Vision Transformers (ViTs), is expanding into sectors such as healthcare, autonomous vehicles, satellite imaging, and more. However, many of these applications need more labelled data, posing a major barrier to achieving high accuracy.</p>
        <p>This workshop will provide a platform to discuss innovative strategies that mitigate data scarcity challenges in Vision Transformers, including but not limited to:</p>
        <ul>
            <li>Data-efficient training of vision transformers – Techniques that reduce the dependence on large datasets for ViTs.</li>
            <li>Self-supervised and unsupervised learning – Exploring methods to pre-train vision transformers using unlabeled data.</li>
            <li>Few-shot and transfer learning – Approaches for transferring knowledge across different tasks and domains with minimal labelled data.</li>
            <li>Model compression and optimisation – Techniques to make vision transformers more lightweight and computationally efficient.</li>
            <li>Applications in resource-constrained environments – ViT-based solutions in healthcare, agriculture, smart cities, and autonomous driving with limited data.</li>
            <li>Benchmarking and evaluation – Analysing performance metrics and benchmarks for data-efficient ViTs.</li>
            <li>Generative models for data augmentation – Using the latest generative models (e.g., StableDiffusion) to create synthetic data for training.</li>    
        </ul>
    </div>
    
    <hr class="orange-line">

    <div class="section" id="format">
        <h2>Format</h2>
        <p>The workshop will consist of:</p>
        <ul>
            <li><strong>Keynote Speakers:</strong> Leading experts in vision transformers and data efficiency will present their latest research and insights.</li>
            <li><strong>Invited Talks:</strong> A selection of presentations from both academia and industry, focusing on the application of data-efficient vision transformers.</li>
            <li><strong>Panel Discussion:</strong> Experts will discuss current challenges and future trends in the field.</li>
            <li><strong>Paper Presentations:</strong> Contributed papers on related topics will be presented, followed by Q&A sessions.</li>
        </ul>
        <h3>Invited Speakers</h3>
        <p>The following experts have been invited to share their insights:</p>
        <ul>
            <li><strong>TBA</strong> - </li>
        </ul>
    </div>
    <hr class="orange-line">  

    <!-- Target Audience Section -->
    <div class="section" id="audience">
        <h2>Target Audience</h2>
        <p>This workshop will attract:</p>
        <ul>
            <li><strong>Researchers and practitioners:</strong> Those in computer vision and deep learning, particularly working with vision transformers.</li>
            <li><strong>AI professionals and data scientists:</strong> Individuals interested in developing efficient models for data-scarce environments.</li>
            <li><strong>Industry professionals:</strong> Stakeholders looking for practical applications of vision transformers in fields like healthcare, autonomous systems, and IoT.</li>
            <li><strong>Graduate students and early-career researchers:</strong> Those exploring new frontiers in neural network-based computer vision.</li>
        </ul>
    </div>
  

    <hr class="orange-line">

    <!-- Submission Guidelines Section -->
    <div class="section" id="submission-guidelines">
        <h2>Submission Guidelines</h2>
        <p>Please follow the instructions carefully for submitting your paper to IJCNN 2025:</p>
        <ul>
            <li><strong style="color: red;">Double-Blind Reviewing:</strong> <span style="font-weight: bold;">Ensure anonymity by removing author names and references to prior work in the first person. Papers revealing author identities may be rejected.</span></li>
        <li><strong style="color: red;">Submission System:</strong> <span style="font-weight: bold;">Papers must be submitted through the IEEE IJCNN 2025 online submission system. Special session papers must select the corresponding title 'Data-efficient Vision Transformers: Challenges & Applications' during submission.</span></li>
            <li><strong>AI-Generated Text Disclosure:</strong> If using AI-generated text, disclose it in the acknowledgements and cite the AI system used.</li>
            <li><strong>Plagiarism Check:</strong> Papers will be checked for plagiarism and may be desk-rejected if suspected of plagiarism.</li>
            <li><strong>Paper Length:</strong> Each paper is limited to 8 pages. Up to 2 additional pages are allowed for a fee of $100 per extra page.</li>            <li><strong>Submission System:</strong> IJCNN 2025 uses Microsoft CMT for paper submission. The system is available at <a href="https://cmt3.research.microsoft.com/IJCNN2025/" target="_blank">Microsoft CMT</a>.</li>
        </ul>
        <p>For more details, visit <a href="https://2025.ijcnn.org/authors/initial-author-instructions" target="_blank">https://2025.ijcnn.org/authors/initial-author-instructions</a>.</p>
    </div>


    <hr class="orange-line">

    <!-- Important Dates Section -->
    <div class="section" id="important-dates">
        <h2>Important Dates</h2>
        <ul>
            <li><strong>15 January 2025:</strong> Paper Submission Deadline</li>
            <li><strong>31 March 2025:</strong> Paper Acceptance Notification</li>
            <li><strong>1 May 2025:</strong> Camera-Ready Paper Submission Deadline</li>
            <li><strong>1 May 2025:</strong> Early Registration Deadline</li>
            <li><strong>TBA:</strong> Special Session: Data-efficient Vision Transformers</li>
        </ul>
    </div>


    <hr class="orange-line">

   <!-- Organizers Section -->
    <div class="section" id="organizers">
        <h2>Organizers</h2>
        <div class="organizers-container">
            <!-- Dr. Haider Raza -->
            <div class="organizer">
                <img src="images/RAZAH72409.jpg" alt="Dr. Haider Raza" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Dr. Haider Raza</p>
                    <p class="affiliation">University of Essex, UK</p>
                    <p class="biography">Dr. Raza is a Senior Lecturer with a strong background in AI, deep learning, and computer vision. His research focuses on developing AI solutions for healthcare, autonomous systems, and digital technology. He has published extensively on efficient AI models and has a track record of organising successful workshops and conferences.</p>
                </div>
            </div>
            
            <!-- Dr. Muhammad Haris Khan -->
            <div class="organizer">
                <img src="images/Muhammad-Haris-Khan-secondary.jpg" alt="Dr. Muhammad Haris Khan" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Dr. Muhammad Haris Khan</p>
                    <p class="affiliation">MBZUAI, UAE</p>
                    <p class="biography">Dr. Khan is an Assistant Professor at MBZUAI with expertise in computer vision, Vision Transformers, and data-efficient learning methods. His research addresses the challenges of model generalizability to new domains, data and label scarcity, and efficiency in AI models.</p>
                </div>
            </div>
            
            <!-- Prof. John Q Gan -->
            <div class="organizer">
                <img src="images/GANJO00207.jpg" alt="Prof. John Q Gan" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Prof. John Q Gan</p>
                    <p class="affiliation">University of Essex, UK</p>
                    <p class="biography">Prof. Gan is a professor of artificial intelligence with extensive experience in deep learning and its applications in image and video classification, medical image analysis, and understanding.</p>
                </div>
            </div>
            
            <!-- Mohsin Ali -->
            <div class="organizer">
                <img src="images/Mohsin.jpeg" alt="Mohsin Ali" class="organizer-img">
                <div class="organizer-info">
                    <p class="name">Mohsin Ali</p>
                    <p class="affiliation">University of Essex, UK</p>
                    <p class="biography">Mohsin Ali is a PhD scholar at the University of Essex, specializing in computer vision with a focus on Vision Transformers (ViTs) and explainable AI. His research aims to enhance the interpretability and efficiency of deep learning models in real-world applications.</p>
                </div>
            </div>
        </div>
    </div>



    <hr class="orange-line">

    <!-- Contact Section -->
    <div class="section" id="contact">
        <h2>Contact Information</h2>
        <p>For inquiries regarding the workshop, feel free to contact:</p>
        <ul>
            <li><strong>Dr. Haider Raza</strong> (Email: <a href="mailto:h.raza@essex.ac.uk">h.raza@essex.ac.uk</a>)</li>
            <li><strong>Dr. Muhammad Haris Khan</strong> (Email: <a href="mailto:muhammad.haris@mbzuai.ac.ae">muhammad.haris@mbzuai.ac.ae</a>)</li>
            <li><strong>Professor John Q Gan</strong> (Email: <a href="mailto:jqgan@essex.ac.uk">jqgan@essex.ac.uk</a>)</li>
        </ul>
        <p>This workshop is co-located at the IJCNN 2025. More details can be found on the official conference website: 
            <a href="https://2025.ijcnn.org/" target="_blank">https://2025.ijcnn.org/</a>
        </p>
    </div>


    <hr class="orange-line">

   <!-- Institutes Section -->
    <div class="section" id="institutes">
        <h2>Institutes</h2>
        <div class="institutes-container">
            <div class="institute">
                <img src="images/University_of_Essex_logo.png" alt="University of Essex" class="institute-img">
                <div class="institute-info">
                    <p class="name">University of Essex</p>
                    <p class="affiliation">Colchester, United Kingdom</p>
                </div>
            </div>
            <div class="institute">
                <img src="images/mbzua_logo.png" alt="MBZUAI" class="institute-img">
                <div class="institute-info">
                    <p class="name">MBZUAI</p>
                    <p class="affiliation">Abu Dhabi, United Arab Emirates</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Conclusion Section -->
    <div class="section" id="conclusion">
        <p>We believe this workshop will offer significant insights and foster discussions on the future of Vision Transformers in data-efficient settings, aligning with the goals of IJCNN 2025. We look forward to contributing to the conference with this engaging and timely topic.</p>
    </div>

    <!-- Footer Section for Copyright -->
    <footer class="copyright-banner">
        <p>&copy; 2025 Special Session @ IJCNN. All Rights Reserved.</p>
    </footer>

</body>
</html>
